{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:center; width:35%\"><img src='https://dl.dropbox.com/s/qtzukmzqavebjd2/icon_smu.jpg' style=\"width: 300px; height: 90px; \"></th>\n",
    "    <th style=\"text-align:center;\"><font size=\"4\"> <br/>IS.215 - Analytics in Python Practical 1</font></th>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Test\n",
    "\n",
    "The data \"liver_patient.csv\" is adapted from Indian Liver Patient dataset (https://www.mldata.io/dataset-details/indian_liver_patient/). It is a binary (2-class) classification problem predicting if the patient has liver disease based on various readings and values. There are 583 observations with 9 input variables and 1 output/target variable.  The column names are 'age', 't_bilirubin', 'd_bilirubin', 'akl_ph', 'ala_at', 'asp_at', 't_protein', 'albumin', 'alb_glo', 'target' and the details as follows: \n",
    "\n",
    "- age: Age of patient.\n",
    "- t_bilirubin: Total reading of Bilirubin.\n",
    "- d_bilirubin: Direct reading of Bilirubin.\n",
    "- akl_ph: Alkaline Phosphotase reading.\n",
    "- ala_at: Alamine Aminotransferase reading.\n",
    "- asp_at: Aspartate Aminotransferase reading.\n",
    "- t_protein: Total amount of proteins.\n",
    "- albumin: Albumin reading.\n",
    "- alb_glo: Ratio of Albumin and Globulin reading.\n",
    "- target: Target variable (1 -'yes, a liver patient' or 2-'no, not a liver patient')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: import relevant libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: read in the data**\n",
    "\n",
    "After importing the libraries, the source file 'diabetes.csv' is imported by making use of read_csv of pandas into dataframe (df). Essentially it is reading the data as dataframe columns with the corresponding name for ease of reference.\n",
    "\n",
    "The preliminary analysis includes checking for dimension of the imported dataframe (df.shape) and making sure that it is 768 rows and 9 columns (including the target column). df.describe() is used to check the summary of the individual column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(583, 10)\n",
      "              age  t_bilirubin  d_bilirubin       akl_ph       ala_at  \\\n",
      "count  583.000000   583.000000   583.000000   583.000000   583.000000   \n",
      "mean    44.746141     3.298799     1.486106   290.576329    80.713551   \n",
      "std     16.189833     6.209522     2.808498   242.937989   182.620356   \n",
      "min      4.000000     0.400000     0.100000    63.000000    10.000000   \n",
      "25%     33.000000     0.800000     0.200000   175.500000    23.000000   \n",
      "50%     45.000000     1.000000     0.300000   208.000000    35.000000   \n",
      "75%     58.000000     2.600000     1.300000   298.000000    60.500000   \n",
      "max     90.000000    75.000000    19.700000  2110.000000  2000.000000   \n",
      "\n",
      "            asp_at   t_protein     albumin       alb_glo      target  \n",
      "count   583.000000  583.000000  583.000000     583.00000  583.000000  \n",
      "mean    109.910806    6.483190    3.141852    -685.16578    1.286449  \n",
      "std     288.918529    1.085451    0.795519    8261.85600    0.452490  \n",
      "min      10.000000    2.700000    0.900000 -100000.00000    1.000000  \n",
      "25%      25.000000    5.800000    2.600000       0.70000    1.000000  \n",
      "50%      42.000000    6.600000    3.100000       0.92000    1.000000  \n",
      "75%      87.000000    7.200000    3.800000       1.10000    2.000000  \n",
      "max    4929.000000    9.600000    5.500000       2.80000    2.000000  \n"
     ]
    }
   ],
   "source": [
    "#Step 2: read in the data and do a preliminary analysis\n",
    "\n",
    "df = pd.read_csv(\"liver_patient.csv\", names=['age', 't_bilirubin', 'd_bilirubin', 'akl_ph', 'ala_at', 'asp_at', 't_protein', 'albumin', 'alb_glo', 'target'])\n",
    "print(df.shape)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: create input and target data**\n",
    "\n",
    "Since the data is read as a whole into df. There is a need to split the content that containing input variables or information regarding the diagnotics values and the target value. The target value is actually the label of the input values, indicating if the person has diabetes or not.\n",
    "\n",
    "input_df = df.drop('class', axis=1) is to create input_df that contains only the first 8 columns containing diagnotics information and removing the last column ('class'). It doesn't make sense to include 'class' in the data to train the model since it is the target we want to learn and predict.\n",
    "\n",
    "target = df['class'] is creating a dataframe that only contains the target value - '0' or '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(583, 9) (583,)\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Split into input_df and target dataframe. axis=0, row, axis=1, column.\n",
    "input_df = df.drop('target', axis=1)\n",
    "target = df['target']\n",
    "\n",
    "print(input_df.shape,target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    416\n",
       "2    167\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distribution of class - imbalance class with '1' having lesser count\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: split data into training and testing data**\n",
    "\n",
    "In order to train a classifier or a classification model, there is a need to split the input data and corresponding target value into two parts - training and testing data. The purpose of having two set of data is to evaluate the model (built using the training data). In other words, we want to be able to evaluate how good is the model by comparing the predicted result from the trained model and the testing data. Scikit-learn provides a useful function - train_test_split that is able to separate two set of data according to a proportion, for example, when test_size=0.3, it means splitting 70% training -30% testing. \n",
    "\n",
    "It is common to use 'X' to denote input values and 'y' to indicate target value in scikit-learn. As a result, X_train, y_train is essentially the input values and corresponding target value of training data. Similarly for X_test and y_test for testing data.\n",
    "\n",
    "Question 1. Split data into 70% training and 30% testing data and keeping random state as 7. How many records (rows) are found in the newly created training data set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 9) (175, 9) (408,) (175,)\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Split feature and label sets to train and data sets - 70-30, random_state is desirable for reproducibility, stratify - same proportion as input data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_df, target, test_size = 0.3, random_state = 7, stratify = target)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the range of input values are of different scale (from df.describe()), there is a need to normalize the values between 0-1 so that the range of values won't influence the model. It is important to do the same scaling and transforming on both input values of training data (X_train) and testing data (X_test) to ensure meaningful evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 - Normalize using MinMaxScaler to constrain values to between 0 and 1.\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: create the Logistic Regression model**\n",
    "\n",
    "The algorithm used in this example is Logistic Regression. It is one of the commonly used classification algorithm. The model created is 'logreg'. You can use any other variable to represent it, e.g., 'lr'. \n",
    "\n",
    "logreg.fit(X_train, y_train) - using the training input data and target value to build the 'logreg' model. After this command, 'logreg' will learn the 'rules'/'knowledge' of differentiating an onset diabetes and non-diabetes patient.\n",
    "\n",
    "y_pred = logreg.predict(X_test) - 'logreg' model is used to predict X_test and the predicted result can be found in y_pred. If the model has learnt all the 'knowledge/rules', y_pred will be 100% match with y_test. \n",
    "\n",
    "The subsequent codes are to evaluate the predicted results using both accuracy and F1-score. You may observe that the recall value is quite low for target value '1', this is likely due to the imbalance class '1' and '0' of the original data with '1' having 268 and '0' having 500 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.72\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.94      0.83       125\n",
      "           2       0.53      0.16      0.25        50\n",
      "\n",
      "    accuracy                           0.72       175\n",
      "   macro avg       0.64      0.55      0.54       175\n",
      "weighted avg       0.68      0.72      0.66       175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Create a logistic regression classifier, default c=1\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "#look at the value under the '1' class (or 'yes') for the corresponding precision, recall and f1 score\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to improve the model is to handle the imbalance class data by oversampling the minority class. The method used in this example is SMOTE which stands for Synthetic Minority Over-sampling Techniques. SMOTE is applied on the X_train and y_train to ensure that both '1' and '0' classes are represented equally.\n",
    "\n",
    "Based on the result, it can be observed that precision and recall are now more balanced (for the 'yes' or '1' class) and this model created is a better model to be used in predicting the onset of diabates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to be able to interpret the model created and to understand the importance of each features. For Logistic Regression, since we have normalized the input values, we can make use of the magnitude of the coefficients (coef_) to find the important features. \n",
    "\n",
    "argsort returns indices of the sorted values from smallest to the largest. By adding a '-' for the logreg.coef_, it will sort in descending order.\n",
    "\n",
    "Feature with largest value contributes the most to the model (based on the magnitude associated with the feature, it is glucose_conc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['albumin' 'alb_glo' 'akl_ph' 'asp_at' 'ala_at' 'age' 't_bilirubin'\n",
      "  't_protein' 'd_bilirubin']]\n"
     ]
    }
   ],
   "source": [
    "#Question 1,3 - get the descending sorted indices based on coefficient values\n",
    "sorted_index = np.argsort(-logreg.coef_)\n",
    "\n",
    "#get the feature_names\n",
    "feature_names = input_df.columns\n",
    "\n",
    "#get the names of the important features (largest to smallest)\n",
    "print(feature_names.to_numpy()[sorted_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Accuracy is a suitable evaluation metric to assess this classifier.\n",
    "\n",
    "\n",
    "\n",
    "Question 4: Which is the most important feature based on the raw data (considering that the coefficient values of logistic regression can be used for the purpose)?\n",
    "\n",
    "`albumin`\n",
    "\n",
    "Question 5: Does scaling or normalization has an impact in the ranking of the important features? Explain your reason.\n",
    "\n",
    "\n",
    "Yes it does. By looking at df.describe(), we can see that alb_glo has a very large range and high standard deviation - the minimum is -100000 and max is 2.80000!\n",
    "\n",
    "The -100000 is most likely an outlier, which will have an impact in the ranking of the features.\n",
    "\n",
    "This, along with all other features, should be scaled down to be between 0 and 1 using a MinMaxScaler to reduce the impact of outliers with very large numbers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
