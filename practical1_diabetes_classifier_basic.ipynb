{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:center; width:35%\"><img src='https://dl.dropbox.com/s/qtzukmzqavebjd2/icon_smu.jpg' style=\"width: 300px; height: 90px; \"></th>\n",
    "    <th style=\"text-align:center;\"><font size=\"4\"> <br/>IS.215 - Analytics in Python Practical 1</font></th>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program builds a classifier for Pima Indians Diabetes dataset - https://www.kaggle.com/uciml/pima-indians-diabetes-database. It is a binary (2-class) classification problem. There are 768 observations with 8 input variables and 1 output/target variable. The variable names are as follows:\n",
    "\n",
    "- Number of times pregnant.\n",
    "- Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "- Diastolic blood pressure (mm Hg).\n",
    "- Triceps skinfold thickness (mm).\n",
    "- 2-Hour serum insulin (mu U/ml).\n",
    "- Body mass index (weight in kg/(height in m)^2).\n",
    "- Diabetes pedigree function.\n",
    "- Age (years).\n",
    "- Target variable (0 -'no' or 1-'yes')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: read in the data and analyse\n",
    "df = pd.read_csv(\"diabetes.csv\", names=['num_pregnant','glucose_conc','diastolic_bp','triceps_thick','serum_insulin','bmi','pedigree','age','class'])\n",
    "print(df.shape)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Split into input and target dataframes. axis=0, row, axis=1, column. \n",
    "input_df = ?\n",
    "target = ?\n",
    "\n",
    "print(input_df.shape,target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of class\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Split feature and label sets to train and data sets - 70-30, random_state is desirable for reproducibility, stratify - same proportion as input data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_df, target, test_size = 0.3, random_state = 10, stratify = target)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 - Normalize using MinMaxScaler to constrain values to between 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "#?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Create a logistic regression classifier, default c=1\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "#look at the value under the positive class - macro avg is insensitive to imbalanced data, micro result will be affected if there is imbalance data - infleunce by majority\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4 - handling imbalance data - Rerunning above with resampled data - using oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state = 2)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print(X_train_sm.shape, X_train.shape)\n",
    "\n",
    "#?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1,3 - analyse the features\n",
    "#get the sorting indices in descending order\n",
    "sorted_index = np.argsort(-logreg.coef_)\n",
    "\n",
    "#get the feature_names\n",
    "feature_names = input_df.columns\n",
    "\n",
    "#get the names of the important features\n",
    "#?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
