{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:center; width:35%\"><img src='https://dl.dropbox.com/s/qtzukmzqavebjd2/icon_smu.jpg' style=\"width: 300px; height: 90px; \"></th>\n",
    "    <th style=\"text-align:center;\"><font size=\"4\"> <br/>IS.215 - Analytics in Python Practical 1</font></th>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program builds a classifier for Pima Indians Diabetes dataset - https://www.kaggle.com/uciml/pima-indians-diabetes-database. It is a binary (2-class) classification problem. There are 768 observations with 8 input variables and 1 output/target variable. The variable names are as follows:\n",
    "\n",
    "- Number of times pregnant.\n",
    "- Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "- Diastolic blood pressure (mm Hg).\n",
    "- Triceps skinfold thickness (mm).\n",
    "- 2-Hour serum insulin (mu U/ml).\n",
    "- Body mass index (weight in kg/(height in m)^2).\n",
    "- Diabetes pedigree function.\n",
    "- Age (years).\n",
    "- Target variable (0 if non-diabetic, 1 if diabetic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "       num_pregnant  glucose_conc  diastolic_bp  triceps_thick  serum_insulin  \\\n",
      "count    768.000000    768.000000    768.000000     768.000000     768.000000   \n",
      "mean       3.845052    120.894531     69.105469      20.536458      79.799479   \n",
      "std        3.369578     31.972618     19.355807      15.952218     115.244002   \n",
      "min        0.000000      0.000000      0.000000       0.000000       0.000000   \n",
      "25%        1.000000     99.000000     62.000000       0.000000       0.000000   \n",
      "50%        3.000000    117.000000     72.000000      23.000000      30.500000   \n",
      "75%        6.000000    140.250000     80.000000      32.000000     127.250000   \n",
      "max       17.000000    199.000000    122.000000      99.000000     846.000000   \n",
      "\n",
      "              bmi    pedigree         age       class  \n",
      "count  768.000000  768.000000  768.000000  768.000000  \n",
      "mean    31.992578    0.471876   33.240885    0.348958  \n",
      "std      7.884160    0.331329   11.760232    0.476951  \n",
      "min      0.000000    0.078000   21.000000    0.000000  \n",
      "25%     27.300000    0.243750   24.000000    0.000000  \n",
      "50%     32.000000    0.372500   29.000000    0.000000  \n",
      "75%     36.600000    0.626250   41.000000    1.000000  \n",
      "max     67.100000    2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Read in the data and analyse\n",
    "df = pd.read_csv(\n",
    "    \"diabetes.csv\",\n",
    "    names=['num_pregnant','glucose_conc','diastolic_bp','triceps_thick','serum_insulin','bmi','pedigree','age','class'])\n",
    "\n",
    "print(df.shape) # returns the dimensions of the data (row x column)\n",
    "print(df.describe()) # summarises the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8) (768,)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Split into input and target dataframes. axis=0, row, axis=1, column. \n",
    "input_df = df.drop(\"class\", axis=1) # we're trying to make the model guess the input, so we remove it first\n",
    "target = df['class'] # target is something like expected output\n",
    "\n",
    "print(input_df.shape, target.shape) # should expect 8 columns cos we dropped the `class` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of class i.e. how many have diabetes and how many doesn't have\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `train_test_split`, see the [documentations](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537, 8) (231, 8) (537,) (231,)\n"
     ]
    }
   ],
   "source": [
    "# Step 4:\n",
    "# Split feature (input_df) and label sets (target) to train and test datasets - 70-30\n",
    "# random_state is desirable for reproducibility (so that the randomness is deterministic)\n",
    "# stratify - same proportion as input data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_df, target, test_size=0.3, random_state=10, stratify=target)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Now, X becomes all the features (e.g. bmi, age, glucose) and y becomes the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Check the range of the values of the features. Are they of similar ranges? Will the ranges affect the prediction result? What should you do?\n",
    "\n",
    "#### Answer\n",
    "I understand [from this Medium article](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02) that `MinMaxScaler` preserves the shape of the original distribution, and it doesn't meaningfully change the information embedded in the original data. In this case, we can use the `MinMaxScaler` to scale down the values:\n",
    "- to be between 0 and 1\n",
    "- while still keeping the original distribution\n",
    "\n",
    "We can use `MinMaxScaler` here as a starting point, if we are not sure we should even standardise to normal distribution using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 - Normalize using MinMaxScaler to constrain values to between 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "\n",
    "# # print(X_train)\n",
    "\n",
    "# Create a model using the training dataset first\n",
    "scaler.fit(X_train)\n",
    "# Then scale down both\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(X_train) # X_train now would be scaled down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8051948051948052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86       150\n",
      "           1       0.81      0.58      0.68        81\n",
      "\n",
      "    accuracy                           0.81       231\n",
      "   macro avg       0.81      0.75      0.77       231\n",
      "weighted avg       0.81      0.81      0.80       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create a logistic regression classifier, default c=1\n",
    "logreg = LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "# This is where you actually train the model, using fit() with the training dataset\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Now, using your trained LogisticRegression model,\n",
    "# we pass in the X_test (features, but test dataset)\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Testing accuracy %s\" % accuracy)\n",
    "\n",
    "# look at the value under the positive class\n",
    "# macro avg is insensitive to imbalanced data\n",
    "# micro result will be affected if there is imbalance data - infleunce by majority\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "This is a 2-classes dataset but it is imbalanced. As a result, there is a possibility that one class is over-represented and the model built maybe biased towards to majority. One approach to solve this is to oversample the smaller data.\n",
    "\n",
    "#### My notes\n",
    "Since the answer is already given above, just writing my notes here: In our training dataset, notice that we have 350 who do not have diabetes and 187 who have â€” it makes our dataset imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing training dataset's counts\n",
      "0    350\n",
      "1    187\n",
      "Name: class, dtype: int64\n",
      "\n",
      "\n",
      "Resampled dataset's counts\n",
      "1    350\n",
      "0    350\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Resampled dataset vs. existing training dataset\n",
      "(700, 8) (537, 8)\n",
      "\n",
      "\n",
      "<bound method ClassifierMixin.score of LogisticRegression(solver='liblinear')>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.72      0.78       150\n",
      "           1       0.59      0.75      0.66        81\n",
      "\n",
      "    accuracy                           0.73       231\n",
      "   macro avg       0.72      0.74      0.72       231\n",
      "weighted avg       0.76      0.73      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 4 - handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Rerunning above with resampled data - using oversampling\n",
    "sm = SMOTE(random_state=2) # synthetic creation of data that's balanced\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train.ravel()) # using 70% of our dataset to train\n",
    "\n",
    "print(\"Existing training dataset's counts\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Resampled dataset's counts\")\n",
    "print(pd.value_counts(pd.Series(y_train_sm)))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Resampled dataset vs. existing training dataset\")\n",
    "print(X_train_sm.shape, X_train.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "clf = logreg.fit(X_train_sm, y_train_sm)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(clf.score)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['glucose_conc' 'bmi' 'pedigree' 'num_pregnant' 'age' 'serum_insulin'\n",
      "  'triceps_thick' 'diastolic_bp']]\n"
     ]
    }
   ],
   "source": [
    "# Question 1, 3 - analyse the features\n",
    "# Get the sorting indices in descending order\n",
    "sorted_index = np.argsort(-logreg.coef_)\n",
    "\n",
    "# Get the feature_names\n",
    "feature_names = input_df.columns\n",
    "\n",
    "# Get the names of the important features\n",
    "print(feature_names.to_numpy()[sorted_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Which feature play an important role in the prediction? Is it explainable?\n",
    "\n",
    "#### Answer\n",
    "I would think `glucose_conc`, `serum_insulin` and `pedigree` are more important features. A [cursory research](https://www.healthhub.sg/a-z/diseases-and-conditions/626/diabetes) shows that glucose tolerance, having sufficient insulin and having a family history (i.e. `pedigree`) would affect the chances of getting diabetes.\n",
    "\n",
    "\n",
    "### Exercise 3\n",
    "Rerun the program and check the features again. Is the result more explainable now?\n",
    "\n",
    "#### Answer\n",
    "Yes, the top three features are:\n",
    "\n",
    "- `glucose_conc`\n",
    "- `bmi`\n",
    "- `pedigree`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is the accuracy? What does it imply?\n",
    "\n",
    "#### Answer\n",
    "The testing accuracy is 80.9%. It implies the percentage in which the model got a prediction correct, out of all the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
