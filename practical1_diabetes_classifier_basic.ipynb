{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:center; width:35%\"><img src='https://dl.dropbox.com/s/qtzukmzqavebjd2/icon_smu.jpg' style=\"width: 300px; height: 90px; \"></th>\n",
    "    <th style=\"text-align:center;\"><font size=\"4\"> <br/>IS.215 - Analytics in Python Practical 1</font></th>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program builds a classifier for Pima Indians Diabetes dataset - https://www.kaggle.com/uciml/pima-indians-diabetes-database. It is a binary (2-class) classification problem. There are 768 observations with 8 input variables and 1 output/target variable. The variable names are as follows:\n",
    "\n",
    "- Number of times pregnant.\n",
    "- Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "- Diastolic blood pressure (mm Hg).\n",
    "- Triceps skinfold thickness (mm).\n",
    "- 2-Hour serum insulin (mu U/ml).\n",
    "- Body mass index (weight in kg/(height in m)^2).\n",
    "- Diabetes pedigree function.\n",
    "- Age (years).\n",
    "- Target variable (0 -'no' or 1-'yes')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "       num_pregnant  glucose_conc  diastolic_bp  triceps_thick  serum_insulin  \\\n",
      "count    768.000000    768.000000    768.000000     768.000000     768.000000   \n",
      "mean       3.845052    120.894531     69.105469      20.536458      79.799479   \n",
      "std        3.369578     31.972618     19.355807      15.952218     115.244002   \n",
      "min        0.000000      0.000000      0.000000       0.000000       0.000000   \n",
      "25%        1.000000     99.000000     62.000000       0.000000       0.000000   \n",
      "50%        3.000000    117.000000     72.000000      23.000000      30.500000   \n",
      "75%        6.000000    140.250000     80.000000      32.000000     127.250000   \n",
      "max       17.000000    199.000000    122.000000      99.000000     846.000000   \n",
      "\n",
      "              bmi    pedigree         age       class  \n",
      "count  768.000000  768.000000  768.000000  768.000000  \n",
      "mean    31.992578    0.471876   33.240885    0.348958  \n",
      "std      7.884160    0.331329   11.760232    0.476951  \n",
      "min      0.000000    0.078000   21.000000    0.000000  \n",
      "25%     27.300000    0.243750   24.000000    0.000000  \n",
      "50%     32.000000    0.372500   29.000000    0.000000  \n",
      "75%     36.600000    0.626250   41.000000    1.000000  \n",
      "max     67.100000    2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Read in the data and analyse\n",
    "df = pd.read_csv(\n",
    "    \"diabetes.csv\",\n",
    "    names=['num_pregnant','glucose_conc','diastolic_bp','triceps_thick','serum_insulin','bmi','pedigree','age','class'])\n",
    "\n",
    "print(df.shape) # returns the dimensions of the data (row x column)\n",
    "print(df.describe()) # summarises the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8) (768,)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Split into input and target dataframes. axis=0, row, axis=1, column. \n",
    "input_df = df.drop(\"class\", axis=1) # we're trying to make the model guess the input, so we remove it first\n",
    "target = df['class'] # target is something like expected output\n",
    "\n",
    "print(input_df.shape, target.shape) # should expect 8 columns cos we dropped the `class` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of class, e.g. how many have diabetes and how many doesn't have\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537, 8) (231, 8) (537,) (231,)\n"
     ]
    }
   ],
   "source": [
    "# Step 4:\n",
    "# Split feature and label sets to train and data sets - 70-30\n",
    "# random_state is desirable for reproducibility\n",
    "# stratify - same proportion as input data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_df, target, test_size=0.3, random_state=10, stratify=target)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 - Normalize using MinMaxScaler to constrain values to between 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "#?\n",
    "\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.8181818181818182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       150\n",
      "           1       0.81      0.63      0.71        81\n",
      "\n",
      "    accuracy                           0.82       231\n",
      "   macro avg       0.82      0.77      0.79       231\n",
      "weighted avg       0.82      0.82      0.81       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Create a logistic regression classifier, default c=1\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\")\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(\"Testing accuracy %s\" % accuracy_score(y_test, y_pred))\n",
    "#look at the value under the positive class - macro avg is insensitive to imbalanced data, micro result will be affected if there is imbalance data - infleunce by majority\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "This is a 2-classes dataset but it is imbalanced. As a result, there is a possibility that one class is over-represented and the model built maybe biased towards to majority. One approach to solve this is to oversample the smaller data.\n",
    "\n",
    "In our dataset, we have 350 who do not have diabetes and 187 who have, which makes our dataset imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing dataset's counts\n",
      "0    350\n",
      "1    187\n",
      "Name: class, dtype: int64\n",
      "\n",
      "\n",
      "Resampled dataset's counts\n",
      "1    350\n",
      "0    350\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Resampled dataset vs. existing dataset\n",
      "(700, 8) (537, 8)\n",
      "\n",
      "\n",
      "<bound method ClassifierMixin.score of LogisticRegression(solver='liblinear')>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.74      0.79       150\n",
      "           1       0.61      0.75      0.67        81\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.73      0.75      0.73       231\n",
      "weighted avg       0.76      0.74      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 4 - handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Rerunning above with resampled data - using oversampling\n",
    "sm = SMOTE(random_state=2) # synthetic creation of data that's balanced\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print(\"Existing dataset's counts\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Resampled dataset's counts\")\n",
    "print(pd.value_counts(pd.Series(y_train_sm)))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Resampled dataset vs. existing dataset\")\n",
    "print(X_train_sm.shape, X_train.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "clf = logreg.fit(X_train_sm, y_train_sm)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(clf.score)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pedigree' 'num_pregnant' 'bmi' 'glucose_conc' 'age' 'serum_insulin'\n",
      "  'triceps_thick' 'diastolic_bp']]\n"
     ]
    }
   ],
   "source": [
    "#Question 1,3 - analyse the features\n",
    "#get the sorting indices in descending order\n",
    "sorted_index = np.argsort(-logreg.coef_)\n",
    "\n",
    "#get the feature_names\n",
    "feature_names = input_df.columns\n",
    "\n",
    "#get the names of the important features\n",
    "print (feature_names.to_numpy()[sorted_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
